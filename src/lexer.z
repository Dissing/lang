struct LexingContext {
  start: u32,
  current: u32,
  line: u32,
  file_span_offset: u32,
  source: *SourceFile,
  tokens: *Token,
  current_token_idx: u32,
  sess: *Session,
  lexeme_buffer: *char,
}

fn is_alphabetic(c: char) -> bool {
  return (c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')
}

fn is_digit(c: char) -> bool {
  return c >= '0' && c <= '9'
}

fn is_alphanumeric(c: char) -> bool {
  return is_alphabetic(c) || is_digit(c)
}

fn is_done_lexing(ctx: *LexingContext) -> bool {
  var source: *SourceFile = ctx.source;
  return ctx.current >= source.length
}

fn peek_token(ctx: *LexingContext, offset: u32) -> char {
  if is_done_lexing(ctx)  return 0
  var source: *SourceFile = ctx.source;
  return source.content[ctx.current + offset]
}

fn advance(ctx: *LexingContext) -> char {
  ctx.current = ctx.current + 1
  var source: *SourceFile = ctx.source;
  return source.content[ctx.current - 1]
}

fn add_token(ctx: *LexingContext, kind: TokenKind, lexeme: Sid) {
  var token: Token;
  token.kind = kind;
  token.lexeme = lexeme;
  token.span.from = ctx.start + ctx.file_span_offset;
  token.span.to = ctx.current + ctx.file_span_offset;
  ctx.tokens[ctx.current_token_idx] = token;
  ctx.current_token_idx = ctx.current_token_idx + 1;
}

fn add_simple_token(ctx: *LexingContext, kind: TokenKind) {
  var null_sid: Sid;
  null_sid.x = 0;
  add_token(ctx, kind, null_sid);
}

fn add_lookahead_conditional_token(ctx: *LexingContext, expect: char, first: TokenKind, second: TokenKind) {
  var kind: TokenKind;
  if peek_token(ctx,0) == expect {
    advance(ctx);
    kind = first;
  } else {
    kind = second;
  }
  var null_sid: Sid;
  null_sid.x = 0;
  add_token(ctx, kind, null_sid);
}

fn read_newline(ctx: *LexingContext) {
  source_file_newline(ctx.source, ctx.start);
}

fn get_lexeme(ctx: *LexingContext, start_offset: u32, end_offset: u32) -> Sid {
  val str_len: u32 = ctx.current - (ctx.start + start_offset + end_offset);
  var source: *SourceFile = ctx.source;
  memcpy(ctx.lexeme_buffer, &source.content[ctx.start + start_offset], str_len);
  ctx.lexeme_buffer[str_len] = 0
  var sess: *Session = ctx.sess;
  var sid: Sid = intern(&sess.interner, ctx.lexeme_buffer);
  //printf("%s -> %u, %u\n", ctx.lexeme_buffer, sid.x, ctx.start);
  return sid;
}

fn single_line_comment(ctx: *LexingContext) {
  while (peek_token(ctx, 0) != '\n' && !is_done_lexing(ctx)) advance(ctx);
}

fn is_keyword(s: *char) -> TokenKind {
  //TODO Replace this with a match

  if !strcmp(s, "break")  return TokenKind::Break;
  if !strcmp(s, "continue")  return TokenKind::Continue;
  if !strcmp(s, "const")  return TokenKind::Continue;
  if !strcmp(s, "defer")  return TokenKind::Defer;
  if !strcmp(s, "enum")  return TokenKind::Enum;
  if !strcmp(s, "else")  return TokenKind::Else;
  if !strcmp(s, "false")  return TokenKind::False;
  if !strcmp(s, "fn")  return TokenKind::Fn;
  if !strcmp(s, "for")  return TokenKind::For;
  if !strcmp(s, "if")  return TokenKind::If;
  if !strcmp(s, "mod") return TokenKind::Mod;
  if !strcmp(s, "return")  return TokenKind::Return;
  if !strcmp(s, "struct")  return TokenKind::Struct;
  if !strcmp(s, "sizeof")  return TokenKind::Sizeof;
  if !strcmp(s, "true")  return TokenKind::True;
  if !strcmp(s, "union")  return TokenKind::Union;
  if !strcmp(s, "use")  return TokenKind::Use;
  if !strcmp(s, "var")  return TokenKind::Var;
  if !strcmp(s, "val")  return TokenKind::Val;
  if !strcmp(s, "while")  return TokenKind::While;
  return TokenKind::Invalid;
}

fn lex_char(ctx: *LexingContext) {

  var source: *SourceFile = ctx.source;
  val start_line: u32 = source.num_lines + 1;

  while (peek_token(ctx, 0) != '\'' || (peek_token(ctx, -1) == '\\' && peek_token(ctx, -2) != '\\')) && !is_done_lexing(ctx) {
    var c: char = advance(ctx);
    if c == '\n'  read_newline(ctx);
  }

  if is_done_lexing(ctx) {
    printf("Unterminated char starting on line %u\n", start_line);
    abort();
  }

  //Consume closing '
  advance(ctx);

  add_token(ctx, TokenKind::Char, get_lexeme(ctx, 1, 1));
}

fn lex_string(ctx: *LexingContext) {

  var source: *SourceFile = ctx.source;

  val start_line: u32 = source.num_lines + 1;

  while (peek_token(ctx, 0) != '"' || (peek_token(ctx, -1) == '\\' && peek_token(ctx, -2) != '\\')) && !is_done_lexing(ctx) {
    var c: char = advance(ctx);
    if c == '\n'  read_newline(ctx);
  }

  if is_done_lexing(ctx) {
    printf("Unterminated string starting on line %u\n", start_line);
    abort();
  }

  //Consume closing "
  advance(ctx);

  add_token(ctx, TokenKind::String, get_lexeme(ctx, 1, 1));
}

fn lex_number(ctx: *LexingContext) {

  while is_digit(peek_token(ctx,0))  advance(ctx);
  var dot_encountered: bool = false;
  if peek_token(ctx,0) == '.' && is_digit(peek_token(ctx,1)) {
    dot_encountered = true;
    advance(ctx);
    while is_digit(peek_token(ctx,0))  advance(ctx);
  }

  var t: TokenKind;
  if dot_encountered  t = TokenKind::Float;
  else  t = TokenKind::Integer;

  add_token(ctx, t, get_lexeme(ctx, 0, 0));
}

fn lex_identifier(ctx: *LexingContext) {
  while (is_alphanumeric(peek_token(ctx, 0)) || peek_token(ctx, 0) == '_') advance(ctx);

  val lexeme: Sid = get_lexeme(ctx, 0, 0);

  var sess: *Session = ctx.sess;
  val lexeme_str: *char = get_str(&sess.interner, lexeme);
  val keyword: TokenKind = is_keyword(lexeme_str);
  if keyword != TokenKind::Invalid  add_simple_token(ctx, keyword);
  else  add_token(ctx, TokenKind::Identifier, lexeme);
}

fn scan_token(ctx: *LexingContext) {
  val c: char = advance(ctx);

  //TODO Implement char literals such that we can replace the raw ASCII values here
  //TODO A match statement would also be quite nice here
  if c == '(' add_simple_token(ctx, TokenKind::LeftParen);
  else if c == ')' add_simple_token(ctx, TokenKind::RightParen);
  else if c == '[' add_simple_token(ctx, TokenKind::LeftBracket);
  else if c == ']' add_simple_token(ctx, TokenKind::RightBracket);
  else if c == '{' add_simple_token(ctx, TokenKind::LeftCurly);
  else if c == '}' add_simple_token(ctx, TokenKind::RightCurly);
  else if c == '+' add_simple_token(ctx, TokenKind::Plus);
  else if c == '*' add_simple_token(ctx, TokenKind::Star);
  else if c == '%' add_simple_token(ctx, TokenKind::Percent);
  else if c == '^' add_simple_token(ctx, TokenKind::Hat);
  else if c == ';' add_simple_token(ctx, TokenKind::Semicolon);
  else if c == ',' add_simple_token(ctx, TokenKind::Comma);
  else if c == '-' add_lookahead_conditional_token(ctx, '>', TokenKind::Arrow, TokenKind::Minus);
  else if c == ':' add_lookahead_conditional_token(ctx, ':', TokenKind::ColonColon, TokenKind::Colon);
  else if c == '=' add_lookahead_conditional_token(ctx, '=', TokenKind::EqualEqual, TokenKind::Equal);
  else if c == '!' add_lookahead_conditional_token(ctx, '=', TokenKind::BangEqual, TokenKind::Bang);
  else if c == '&' add_lookahead_conditional_token(ctx, '&', TokenKind::AndAnd, TokenKind::And);
  else if c == '|' add_lookahead_conditional_token(ctx, '|', TokenKind::OrOr, TokenKind::Or);
  else if c == '.' {
    if peek_token(ctx, 0) == '.' && peek_token(ctx, 1) == '.' {
      advance(ctx); advance(ctx);
      add_simple_token(ctx, TokenKind::Ellipsis);
    }
    else  add_simple_token(ctx, TokenKind::Dot);
  }
  else if c == '<' {
     val n1: char = peek_token(ctx, 0);
     if n1 == '<' {advance(ctx); add_simple_token(ctx, TokenKind::LessLess);}
     else if n1 == '=' {advance(ctx); add_simple_token(ctx, TokenKind::LessEqual);}
     else add_simple_token(ctx, TokenKind::Less);
  }
  else if c == '>' {
     val n2: char = peek_token(ctx, 0);
     if n2 == '>' {advance(ctx); add_simple_token(ctx, TokenKind::GreaterGreater);}
     else if n2 == '=' {advance(ctx); add_simple_token(ctx, TokenKind::GreaterEqual);}
     else add_simple_token(ctx, TokenKind::Greater);
  }
  else if c == '/' {
    if peek_token(ctx, 0) == '/' single_line_comment(ctx);
    else add_simple_token(ctx, TokenKind::Slash);
  }
  else if c == ' ' || c ==  '\t' || c == '\r' {}
  else if c == '\n' read_newline(ctx);
  else if c == '"' lex_string(ctx);
  else if c == '\'' lex_char(ctx);
  else {
    if is_digit(c) lex_number(ctx);
    else if (is_alphabetic(c) || c == '_') lex_identifier(ctx);
    else {
      var source: *SourceFile = ctx.source;
      printf("Unexpected character %c = %d on line %u\n", c, c, source.num_lines);
      abort();
    }
  }
}

fn lex(sess: *Session, source: *SourceFile, num_tokens: *u32) -> *Token {

  var ctx: *LexingContext = malloc(sizeof(LexingContext));
  ctx.source = source;
  ctx.file_span_offset = source.start;
  ctx.start = 0;
  ctx.current = 0;
  ctx.tokens = malloc(sizeof(Token) * 10000);
  ctx.current_token_idx = 0;
  ctx.sess = sess;

  ctx.lexeme_buffer = malloc(1024);

  while !is_done_lexing(ctx) {
    ctx.start = ctx.current;
    scan_token(ctx);
  }

  source_file_done(source, ctx.current);

  if ctx.current_token_idx > 10000  {
    printf("Lexer token overflow: %u!\n", ctx.current_token_idx);
    abort();
  };

  *num_tokens = ctx.current_token_idx;
  return ctx.tokens;
}
